{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rosafilgueira/PythonCloneDetection/blob/main/Vector_Aggregation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQo1H-ZfAnnr"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJj9rBpvGv3z",
        "outputId": "9737df45-0396-4983-c39c-58948157b3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.8/dist-packages (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.26.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Python 3.8.10\n"
          ]
        }
      ],
      "source": [
        " %pip install transformers scikit-learn\n",
        " !pip install -U sentence-transformers\n",
        " !python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " %pip install transformers datasets tqdm scikit-learn"
      ],
      "metadata": {
        "id": "Ih3TYlCXT0Zp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b42686-3088-4c57-b53a-e2aa1bba3219"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: tokenizers, xxhash, urllib3, multiprocess, responses, huggingface-hub, transformers, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.9.0 huggingface-hub-0.12.0 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.2 transformers-4.26.0 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import astunparse\n",
        "from scipy.optimize import linesearch\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import inspect\n",
        "import ast\n",
        "import astunparse\n",
        "\n",
        "\n",
        "# function to extract source code lines of each function \n",
        "def extract_function_source_code(file_name):\n",
        "    func=[]\n",
        "    with open(file_name, 'r') as file:\n",
        "        source_code = file.read() \n",
        "        parsed_code = ast.parse(source_code)\n",
        "        function_list = [node for node in ast.walk(parsed_code) if isinstance(node, ast.FunctionDef)]\n",
        "        for f_obj in function_list:\n",
        "             data=astunparse.unparse(f_obj)\n",
        "             func.append(data)\n",
        "        return func\n",
        "\n",
        "def create_repo_embeddings(repo_path):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "  model = AutoModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "  \n",
        " \n",
        " # Initialize the list to store the file embeddings\n",
        "  # Iterate through all the source files in the repository\n",
        "\n",
        "  files_embedding=[]\n",
        "  for file in os.listdir(repo_path):\n",
        "    f_embedding=[]\n",
        "    # Extract the source code from each file\n",
        "    functions = extract_function_source_code(os.path.join(repo_path, file))\n",
        "    for f_code in functions: \n",
        "      #print(f_code)\n",
        "      # Generate the embedding for each function\n",
        "      code_tokens=tokenizer.tokenize(f_code)\n",
        "      #print(code_tokens)\n",
        "      tokens=[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "      #print(tokens)\n",
        "      tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "      #print(tokens_ids)\n",
        "      embedding=model(torch.tensor(tokens_ids)[None,:])[0][0,0]\n",
        "      # Append the embedding to the list\n",
        "      f_embedding.append(embedding)\n",
        "    if len(f_embedding) > 0:\n",
        "        f = torch.mean(torch.stack(f_embedding), dim=0)\n",
        "        files_embedding.append(f)\n",
        "     \n",
        "  # Aggregate the file embeddings to get the repo code embedding\n",
        "  repo_embedding = torch.mean(torch.stack(files_embedding), dim=0)\n",
        "  return repo_embedding\n",
        "\n",
        "repo1_embedding=create_repo_embeddings(\"./repo\")\n",
        "repo2_embedding=create_repo_embeddings(\"./repo2\")\n",
        "repo3_embedding=create_repo_embeddings(\"./repo3\")\n",
        "repo4_embedding=create_repo_embeddings(\"./repo4\")\n",
        "\n",
        "#cos = torch.nn.CosineSimilarity(dim=0)\n",
        "\n",
        "output = nn.CosineSimilarity(dim=0, eps=1e-6)(repo1_embedding, repo2_embedding)\n",
        "print(\"Cosine Similarity between repo1 and repo2: %s\" % output)\n",
        "\n",
        "\n",
        "output_2 = nn.CosineSimilarity(dim=0, eps=1e-6)(repo1_embedding, repo3_embedding)\n",
        "print(\"Cosine Similarity between repo1 and repo3: %s\" % output_2)\n",
        "\n",
        "output_3 = nn.CosineSimilarity(dim=0, eps=1e-6)(repo2_embedding, repo4_embedding)\n",
        "print(\"Cosine Similarity between repo2 and repo4: %s\" % output_3)\n",
        "\n",
        "#output = cos(repo1_embedding, repo2_embedding)\n",
        "#print(\"Cosine Similarity between repo1 and repo2: %s\" % output)\n",
        "#output2 = cos(repo1_embedding, repo3_embedding)\n",
        "#print(\"Cosine Similarity between repo1 and repo3: %s\" % output2)\n",
        "#output3 = cos(repo4_embedding, repo3_embedding)\n",
        "#print(\"Cosine Similarity between repo4 and repo3: %s\" %output3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62d0V_M9W0d2",
        "outputId": "d89467cd-1ddf-4a65-a4a6-bebb836804e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between repo1 and repo2: tensor(1.0000, grad_fn=<SumBackward1>)\n",
            "Cosine Similarity between repo1 and repo3: tensor(0.9037, grad_fn=<SumBackward1>)\n",
            "Cosine Similarity between repo2 and repo4: tensor(0.9292, grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf ./repo1/.ipynb_checkpoints\n"
      ],
      "metadata": {
        "id": "kYaz9XVyEbkV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import astunparse\n",
        "from scipy.optimize import linesearch\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import inspect\n",
        "import ast\n",
        "import astunparse\n",
        "from sklearn.decomposition import PCA\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# function to extract source code lines of each function \n",
        "def extract_function_source_code(file_name):\n",
        "    func=[]\n",
        "    with open(file_name, 'r') as file:\n",
        "        source_code = file.read() \n",
        "        parsed_code = ast.parse(source_code)\n",
        "        function_list = [node for node in ast.walk(parsed_code) if isinstance(node, ast.FunctionDef)]\n",
        "        for f_obj in function_list:\n",
        "             data=astunparse.unparse(f_obj)\n",
        "             func.append(data)\n",
        "        return func\n",
        "\n",
        "def create_repo_embeddings(repo_path):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "  model = AutoModel.from_pretrained(\"microsoft/graphcodebert-base\")\n",
        "  \n",
        " \n",
        " # Initialize the list to store the file embeddings\n",
        "  # Iterate through all the source files in the repository\n",
        "\n",
        "  files_embedding=[]\n",
        "  for file in os.listdir(repo_path):\n",
        "    f_embedding=[]\n",
        "    # Extract the source code from each file\n",
        "    functions = extract_function_source_code(os.path.join(repo_path, file))\n",
        "    for f_code in functions: \n",
        "      #print(f_code)\n",
        "      # Generate the embedding for each function\n",
        "      code_tokens=tokenizer.tokenize(f_code, max_length=50,padding='max_length', truncation=True)\n",
        "      #print(code_tokens)\n",
        "      tokens=[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "      #print(tokens)\n",
        "      tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "      #print(tokens_ids)\n",
        "      embedding=model(torch.tensor(tokens_ids)[None,:])[0][0,0]\n",
        "      #print(\"the embedding shape\")\n",
        "      #print(embedding.shape)\n",
        "      f_embedding.append(embedding)\n",
        "    if len(f_embedding) > 0:\n",
        "        f= torch.stack(f_embedding, dim=0)\n",
        "        print(\"---the f-stack shape BEFORE\")\n",
        "        print(f.shape)\n",
        "        files_embedding.append(f)\n",
        "        #print(files_embedding.shape)\n",
        "     \n",
        "  # Aggregate the file embeddings to get the repo code embedding\n",
        "  repo_embedding = torch.cat(files_embedding, dim=0) \n",
        "  print(\"--- the repo-stack shape BEFORE PCA--\")\n",
        "  print(repo_embedding.shape)\n",
        "\n",
        "  #repo_resize = repo_embedding.view(40, 768)\n",
        "  #repo_resize = F.pad(input=repo_embedding, pad=(0, 0), mode='constant', value=0)\n",
        "  #repo_resize = nn.ConstantPad1d((40 - repo_embedding.size()[0],0) ,0 )(repo_embedding)\n",
        "  \n",
        "  padder = torch.zeros(40 - repo_embedding.size()[0], 768)\n",
        "  repo_resize = torch.cat([repo_embedding,padder], dim = 0) \n",
        "\n",
        "  print(repo_resize)\n",
        "  print(\"--- the repo-resize shape BEFORE PCA--\")\n",
        "  print(repo_resize.shape)\n",
        "  repo_dim=repo_resize.size()\n",
        "\n",
        "  num_comp=min(repo_dim[0],repo_dim[1])\n",
        "  print(\"Num_comp %s\" % num_comp)\n",
        "\n",
        "  # Append the embedding to the list\n",
        "  # f_embedding.append(pca_embedding)\n",
        "  X1 = repo_resize.cpu().detach().numpy()\n",
        "  pca = PCA(n_components = num_comp)\n",
        "  X1 = pca.fit_transform(X1)\n",
        "  repo_pca = torch.from_numpy(X1)\n",
        "\n",
        "  print(\"the final repo shape AFTER PCA--\")\n",
        "  print(repo_pca.shape)\n",
        "  return repo_pca\n",
        "\n",
        "repo1_embedding=create_repo_embeddings(\"./repo\")\n",
        "repo2_embedding=create_repo_embeddings(\"./repo1\")\n",
        "\n",
        "\n",
        "cos = torch.nn.CosineSimilarity(dim=1)\n",
        "output = cos(repo1_embedding, repo2_embedding)\n",
        "print(\"Cosine Similarity between repo1 and repo2: %s\" % output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "603wCtDShbj5",
        "outputId": "6dc6cb1c-33c1-4b31-d46c-3a0fa2425bef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---the f-stack shape BEFORE\n",
            "torch.Size([2, 768])\n",
            "---the f-stack shape BEFORE\n",
            "torch.Size([1, 768])\n",
            "---the f-stack shape BEFORE\n",
            "torch.Size([1, 768])\n",
            "--- the repo-stack shape BEFORE PCA--\n",
            "torch.Size([4, 768])\n",
            "tensor([[ 0.1580, -0.6669,  0.1874,  ...,  0.1207, -0.1431,  0.6867],\n",
            "        [ 0.0469, -0.3738,  0.1185,  ...,  0.0479,  0.0517,  0.2263],\n",
            "        [ 0.0652, -0.4650, -0.0373,  ..., -0.0201, -0.0497,  0.4484],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "--- the repo-resize shape BEFORE PCA--\n",
            "torch.Size([40, 768])\n",
            "Num_comp 40\n",
            "the final repo shape AFTER PCA--\n",
            "torch.Size([40, 40])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---the f-stack shape BEFORE\n",
            "torch.Size([10, 768])\n",
            "---the f-stack shape BEFORE\n",
            "torch.Size([6, 768])\n",
            "---the f-stack shape BEFORE\n",
            "torch.Size([4, 768])\n",
            "---the f-stack shape BEFORE\n",
            "torch.Size([4, 768])\n",
            "---the f-stack shape BEFORE\n",
            "torch.Size([3, 768])\n",
            "---the f-stack shape BEFORE\n",
            "torch.Size([7, 768])\n",
            "---the f-stack shape BEFORE\n",
            "torch.Size([6, 768])\n",
            "--- the repo-stack shape BEFORE PCA--\n",
            "torch.Size([40, 768])\n",
            "tensor([[ 0.1336, -0.7445,  0.3154,  ..., -0.0855, -0.1292,  0.3474],\n",
            "        [ 0.0901, -0.3895,  0.1428,  ..., -0.0715,  0.0547,  0.2567],\n",
            "        [ 0.1244, -0.3469,  0.1493,  ..., -0.0640,  0.0600,  0.3892],\n",
            "        ...,\n",
            "        [ 0.0700, -0.4424,  0.0748,  ..., -0.0348, -0.0222,  0.2199],\n",
            "        [ 0.1304, -0.0683,  0.0717,  ..., -0.5634, -0.4308,  0.2660],\n",
            "        [ 0.0640, -0.4560,  0.0659,  ..., -0.0450, -0.0067,  0.2299]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "--- the repo-resize shape BEFORE PCA--\n",
            "torch.Size([40, 768])\n",
            "Num_comp 40\n",
            "the final repo shape AFTER PCA--\n",
            "torch.Size([40, 40])\n",
            "Cosine Similarity between repo1 and repo2: tensor([ 0.3683, -0.6417, -0.7492, -0.7166,  0.6966, -0.3285,  0.3948,  0.8012,\n",
            "        -0.6021, -0.8270, -0.0885,  0.7572,  0.7857,  0.7553,  0.6966, -0.3285,\n",
            "        -0.0885,  0.7572,  0.7857,  0.7553, -0.0885,  0.7572,  0.7857,  0.8048,\n",
            "         0.7085, -0.6170,  0.6676, -0.0885,  0.7572,  0.7553,  0.8177,  0.5948,\n",
            "         0.5982,  0.7127, -0.8804,  0.6587, -0.8851,  0.7170, -0.8914,  0.7168])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " print(\"--- the repo-stack shape BEFORE PCA--\")\n",
        "  print(repo_embedding.shape)\n",
        "  repo_dim=repo_embedding.size()\n",
        "  num_comp=min(repo_dim[0],repo_dim[1])\n",
        "\n",
        "  # Append the embedding to the list\n",
        "  # f_embedding.append(pca_embedding)\n",
        "  X1 = repo_embedding.cpu().detach().numpy()\n",
        "  pca = PCA(n_components = num_comp)\n",
        "  X1 = pca.fit_transform(X1)\n",
        "  repo_pca = torch.from_numpy(X1)\n",
        "\n",
        "  print(\"the final repo shape AFTER PCA--\")\n",
        "  print(repo_pca.shape)\n",
        "  return repo_embedding"
      ],
      "metadata": {
        "id": "8BtZDM9iXYDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# cosine similarity = normalize the vectors & multiply\n",
        "C = F.normalize(repo1_embedding) @ F.normalize(repo2_embedding).t()\n",
        "print(C.shape)\n",
        "print(repo1_embedding.shape)\n",
        "print(repo2_embedding.shape)\n",
        "\n",
        "a_norm = torch.nn.functional.normalize(repo1_embedding, p=2, dim=1)\n",
        "b_norm = torch.nn.functional.normalize(repo2_embedding, p=2, dim=1)\n",
        "d= torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbpsNlYbTjhh",
        "outputId": "99ab1932-088b-4744-806e-7925aed6703a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 40])\n",
            "torch.Size([4, 768])\n",
            "torch.Size([40, 768])\n",
            "tensor([[0.9402, 0.9189, 0.9299, 0.9230, 0.9689, 0.9012, 0.9639, 0.9316, 0.7989,\n",
            "         0.4440, 0.9402, 0.9189, 0.9299, 0.9230, 0.9689, 0.9012, 0.9402, 0.9189,\n",
            "         0.9299, 0.9230, 0.9402, 0.9189, 0.9299, 0.9424, 0.9150, 0.7296, 0.9092,\n",
            "         0.9402, 0.9189, 0.9230, 0.9308, 0.9685, 0.9086, 0.9245, 0.4014, 0.9545,\n",
            "         0.4058, 0.9161, 0.4100, 0.9148],\n",
            "        [0.8716, 0.9897, 0.9817, 0.9822, 0.9489, 0.8130, 0.9227, 0.9814, 0.7090,\n",
            "         0.4386, 0.8716, 0.9897, 0.9817, 0.9822, 0.9489, 0.8130, 0.8716, 0.9897,\n",
            "         0.9817, 0.9822, 0.8716, 0.9897, 0.9817, 0.9730, 0.9829, 0.6504, 0.9893,\n",
            "         0.8716, 0.9897, 0.9822, 0.9800, 0.9340, 0.9927, 0.9893, 0.3804, 0.9465,\n",
            "         0.4114, 0.9854, 0.4137, 0.9858],\n",
            "        [0.8815, 0.9797, 0.9848, 0.9805, 0.9628, 0.8210, 0.9343, 0.9826, 0.7187,\n",
            "         0.4322, 0.8815, 0.9797, 0.9848, 0.9805, 0.9628, 0.8210, 0.8815, 0.9797,\n",
            "         0.9848, 0.9805, 0.8815, 0.9797, 0.9848, 0.9810, 0.9757, 0.6522, 0.9724,\n",
            "         0.8815, 0.9797, 0.9805, 0.9782, 0.9457, 0.9765, 0.9846, 0.3748, 0.9615,\n",
            "         0.3997, 0.9767, 0.4052, 0.9772],\n",
            "        [0.8815, 0.9797, 0.9848, 0.9805, 0.9628, 0.8210, 0.9343, 0.9826, 0.7187,\n",
            "         0.4322, 0.8815, 0.9797, 0.9848, 0.9805, 0.9628, 0.8210, 0.8815, 0.9797,\n",
            "         0.9848, 0.9805, 0.8815, 0.9797, 0.9848, 0.9810, 0.9757, 0.6522, 0.9724,\n",
            "         0.8815, 0.9797, 0.9805, 0.9782, 0.9457, 0.9765, 0.9846, 0.3748, 0.9615,\n",
            "         0.3997, 0.9767, 0.4052, 0.9772]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "CloneDetection",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "70094246f6d6681ad057add4f3fbfe4a8e3e6260a3d82cef639e9b0f5904dda4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}